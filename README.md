# 마나토끼 멀티스레드 이미지 크롤러

이 프로젝트는 지정된 만화 목록 페이지에서 모든 에피소드의 이미지를 병렬로 다운로드하는 Python 스크립트입니다. SeleniumBase를 사용하여 브라우저를 자동화하고, 멀티스레딩을 통해 크롤링 속도를 향상시킵니다.

## 주요 기능

- **멀티스레드 지원**: 여러 페이지를 동시에 크롤링하여 다운로드 시간을 단축합니다.
- **SeleniumBase(uc=True) 사용**: Cloudflare와 같은 봇 탐지 시스템을 우회하도록 시도합니다.
- **동적 페이지 로딩 처리**: 페이지를 아래로 스크롤하여 모든 이미지가 완전히 로드되도록 보장합니다.
- **크롤링 기록 관리**: `database.py`와 연동하여 이미 다운로드한 URL은 건너뛰어 중복 작업을 방지합니다.
- **캡챠 처리**: 캡챠 페이지가 감지되면 스크립트가 일시 중지하고, 사용자가 브라우저에서 직접 캡챠를 해결하도록 안내합니다.

## 요구 사항

- Python 3.x
- `requests`
- `beautifulsoup4`
- `seleniumbase`

## 설치

1.  프로젝트를 클론하거나 다운로드합니다.
2.  필요한 라이브러리를 설치합니다.

    ```bash
    pip install requests beautifulsoup4 seleniumbase
    ```

    또는 프로젝트에 `requirements.txt` 파일이 있다면 다음 명령어를 사용하세요.

    ```bash
    pip install -r requirements.txt
    ```

## 사용법

이 프로젝트는 두 가지 버전의 크롤러를 제공합니다: **CLI(명령줄 인터페이스)** 버전과 **GUI(그래픽 사용자 인터페이스)** 버전입니다.

### 1. GUI 버전 (권장)

GUI 버전은 사용자 친화적인 인터페이스를 통해 더 쉽게 크롤러를 사용할 수 있도록 합니다.

1.  터미널에서 다음 명령어를 실행하여 GUI 애플리케이션을 시작합니다.

    ```bash
    python crawler_gui_multi_multi.py
    ```

2.  애플리케이션 창이 나타나면 다음을 설정합니다:
    - **URL**: 크롤링할 만화 **목록 페이지**의 전체 주소를 입력합니다.
    - **URL TYPE**: 현재는 '목록'만 지원합니다.
    - **스레드 개수**: 동시에 실행할 크롤링 스레드 수를 입력합니다. (기본값: 3, 권장: 1~3)

3.  **시작** 버튼을 클릭하면 크롤링이 시작됩니다. 진행 상황은 하단의 로그 창과 진행률 표시줄을 통해 확인할 수 있습니다.

4.  **중지** 버튼을 누르면 진행 중인 모든 크롤링 작업을 안전하게 중단할 수 있습니다.

### 2. CLI 버전

1.  터미널 또는 명령 프롬프트에서 다음 명령어를 실행합니다.

    ```bash
    python crawler_multi_from_new.py
    ```

2.  스크립트가 실행되면 크롤링할 **만화 목록 페이지**의 URL을 입력하라는 메시지가 나타납니다.

3.  URL을 입력하고 Enter 키를 누르면 확인 메시지가 표시됩니다. 다시 Enter 키를 누르면 크롤링이 시작됩니다.

4.  다운로드된 이미지는 `download_mana` 폴더 내에 각 에피소드 제목으로 생성된 하위 폴더에 저장됩니다.

## 설정

### 스레드 개수 조절

`crawler_multi_from_new.py` 파일 상단에 있는 `NUM_THREADS` 변수의 값을 수정하여 동시에 실행할 스레드 수를 조절할 수 있습니다.

```python
# 스레드 4개부터 IP block 됨.
NUM_THREADS = 3
```

**주의**: 스레드 수를 너무 높게 설정하면 대상 웹사이트의 정책에 따라 IP가 일시적으로 차단될 수 있습니다. 적절한 값을 설정하여 사용하세요.

## 주의사항

- 이 스크립트는 교육 및 개인적인 학습 목적으로만 사용해야 합니다.
- 웹사이트의 서비스 약관(ToS)을 항상 준수하고, 과도한 요청으로 서버에 부담을 주지 않도록 책임감 있게 사용해 주세요.
- 크롤링 중 캡챠가 나타나면 프로그램이 터미널에 안내 메시지를 출력하며 일시 중지됩니다. 이때 열려 있는 브라우저에서 직접 캡챠를 해결한 후, 터미널에서 Enter 키를 눌러야 작업이 재개됩니다.
