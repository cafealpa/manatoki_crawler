import concurrent.futures
import mimetypes
import os
import random
import threading
import time
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog

import requests
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from seleniumbase import Driver

from database import is_url_crawled, add_crawled_url

# --- Global Variables ---
master_thread = None
stop_event = threading.Event()


# crawl_queue = queue.Queue()

# --- Logging Function ---
def log(message):
    """Inserts a message into the log text area in a thread-safe way."""
    log_text.insert(tk.END, message + "\n")
    log_text.see(tk.END)
    root.update_idletasks()


def browse_directory():
    """Opens a dialog to select a download directory."""
    path = filedialog.askdirectory()
    if path:
        download_path_entry.delete(0, tk.END)
        download_path_entry.insert(0, path)


# --- Crawler Functions ---
def scroll_to_bottom_with_pagedown(driver, max_scrolls=80, sleep_time=0.2):
    # log("í˜ì´ì§€ì˜ ëê¹Œì§€ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    body = driver.find_element(By.TAG_NAME, "body")
    scroll_count = 0
    while scroll_count < max_scrolls and not stop_event.is_set():
        last_scroll_y = driver.execute_script("return window.scrollY")
        body.send_keys(Keys.PAGE_DOWN)
        scroll_count += 1
        time.sleep(sleep_time)
        new_scroll_y = driver.execute_script("return window.scrollY")
        if new_scroll_y == last_scroll_y:
            # log("í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ì— ë„ë‹¬í•˜ì—¬ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
    else:
        log(f"ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜({max_scrolls})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
    time.sleep(2)


def handle_captcha(driver, worker_id):
    while "bbs/captcha.php" in driver.current_url and not stop_event.is_set():
        log(f"ì›Œì»¤ {worker_id}: !! ìº¡ì±  í˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ !! ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í•´ê²°í•´ì£¼ì„¸ìš”.")
        while "bbs/captcha.php" in driver.current_url:
            time.sleep(5)
            if stop_event.is_set(): return
        log(f"ì›Œì»¤ {worker_id}: ìº¡ì± ê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        time.sleep(2)


def crawl_worker(worker_id, base_download_path, url_list):
    """The function each thread will execute to crawl pages."""
    time.sleep(worker_id * 5)  # Stagger driver initialization
    driver = None
    try:

        result = []

        driver = Driver(uc=True, headless=False)
        while not stop_event.is_set():
            for url in url_list:
                # url = crawl_queue.get(timeout=1)
                if len(url_list) == 0:
                    continue  # Check stop_event again

                try:
                    # if is_url_crawled(url):
                    #     log(f"ì›Œì»¤ {worker_id}: ì´ë¯¸ í¬ë¡¤ë§ëœ URLì…ë‹ˆë‹¤: {url}")
                    #     continue

                    log(f"ì›Œì»¤ {worker_id}: Navigating to: {url}")
                    driver.get(url)

                    WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
                    )

                    handle_captcha(driver, worker_id)
                    if stop_event.is_set(): break

                    scroll_to_bottom_with_pagedown(driver)

                    page_source = driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')

                    title_element = soup.find('h1') or soup.find('div', class_='view-title')
                    if title_element:
                        post_title = title_element.get_text(strip=True)
                        if "ë§ˆë‚˜í† ë¼ -" in post_title:
                            post_title = post_title.replace(" > ë§ˆë‚˜í† ë¼ - ì¼ë³¸ë§Œí™” í—ˆë¸Œ", "").strip()
                    else:
                        post_title = f"untitled_post_{random.randint(1000, 9999)}"
                    log(f"ì›Œì»¤ {worker_id}: Post Title: {post_title}")

                    download_dir = os.path.join(base_download_path, post_title)
                    os.makedirs(download_dir, exist_ok=True)

                    html_mana_section = soup.find('section', itemtype='http://schema.org/NewsArticle')
                    if html_mana_section:
                        img_tags = html_mana_section.find_all('img')
                        log(f"ì›Œì»¤ {worker_id}: Found {len(img_tags)} images.")

                        for i, img in enumerate(img_tags):
                            if stop_event.is_set(): break
                            img_url = img.get('src')
                            if img_url and '.gif' not in img_url.lower():
                                try:
                                    header = {'referer': 'https://manatoki468.net/'}
                                    response = requests.get(img_url, stream=True, headers=header, timeout=30)
                                    response.raise_for_status()

                                    content_type = response.headers.get('Content-Type')
                                    ext = mimetypes.guess_extension(content_type) or os.path.splitext(img_url)[1] or ".jpg"
                                    img_filename = os.path.join(download_dir, f"{i + 1:03d}{ext}")
                                    with open(img_filename, 'wb') as f:
                                        f.write(response.content)
                                    # log(f"ì›Œì»¤ {worker_id}: Downloaded {img_filename}")
                                except requests.exceptions.RequestException as req_err:
                                    log(f"ì›Œì»¤ {worker_id}: Error downloading {img_url}: {req_err}")

                        if not stop_event.is_set():
                            add_crawled_url(url, post_title)
                            log(f"ì›Œì»¤ {worker_id}: [SUCCESS] í¬ë¡¤ë§ ì™„ë£Œ - {post_title}")
                            result.append({"state": "SUCCESS", "message": "ì„±ê³µ", "title": post_title, "url": url})
                    else:
                        log(f"ì›Œì»¤ {worker_id}: [FAIL] mana_sectionì´ ì—†ìŠµë‹ˆë‹¤. {url}")
                        result.append({"state": "STOPED", "message": "mana_sectionì´ ì—†ìŠµë‹ˆë‹¤.", "title": post_title, "url": url})

                except Exception as e:
                    log(f"ì›Œì»¤ {worker_id}: [FAIL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ {url}. {e}")
                    result.append({"state": "FAIL", "message": f"[FAIL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {e}", "title": post_title, "url": url})
        # future ë°˜í™˜
        return result
    finally:
        if driver:
            driver.quit()
        log(f"ì›Œì»¤ {worker_id}: ì¢…ë£Œ")


def get_target_pages(driver, target_url):
    try:
        log(f"í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤: {target_url}")
        driver.get(target_url)
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
        )
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        article_body = soup.find('article', itemprop='articleBody')
        if article_body:
            serial_list_div = article_body.find('div', class_='serial-list')
            links = serial_list_div.find_all('a', href=True) if serial_list_div else []
            article_urls = [link['href'] for link in links]
            log(f"ì´ {len(article_urls)}ê°œì˜ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return article_urls
        else:
            log("ë§Œí™” ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
    except Exception as e:
        log(f"ëª©ë¡ í˜ì´ì§€ ë¡œë”© ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []


def master_crawl_thread():
    target_url = url_entry.get()
    if not target_url:
        messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    download_path = download_path_entry.get()
    if not download_path:
        download_path = "download_mana"
        log(f"ë‹¤ìš´ë¡œë“œ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ í´ë” '{download_path}'ì— ì €ì¥í•©ë‹ˆë‹¤.")
    os.makedirs(download_path, exist_ok=True)

    crawl_type = url_type.get()
    if crawl_type != "ëª©ë¡":
        messagebox.showinfo("ì•Œë¦¼", "í˜„ì¬ 'ëª©ë¡' ìœ í˜•ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
        return

    try:
        num_threads = int(num_threads_entry.get())
        if not 1 <= num_threads <= 10:
            raise ValueError()
    except ValueError:
        messagebox.showerror("ì˜¤ë¥˜", "ìŠ¤ë ˆë“œ ê°œìˆ˜ëŠ” 1ì—ì„œ 10 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return

    log("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    start_button.config(state=tk.DISABLED)
    stop_button.config(state=tk.NORMAL)
    progress_bar['value'] = 0
    stop_event.clear()

    # Clear queue from previous runs
    # while not crawl_queue.empty():
    #     crawl_queue.get_nowait()

    list_driver = None
    try:
        list_driver = Driver(uc=True, headless=False)
        article_urls = get_target_pages(list_driver, target_url)
    finally:
        if list_driver:
            list_driver.quit()

    if not article_urls:
        log("í¬ë¡¤ë§í•  ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        start_button.config(state=tk.NORMAL)
        stop_button.config(state=tk.DISABLED)
        return

    # ìˆ˜ì§‘ëœ URL ì¤‘ ì´ë¯¸ ìˆ˜ì§‘ëœ URL ì œì™¸
    total_articles = len(article_urls)
    crawled_urls = 0
    target_article_urls = []

    for url in article_urls:
        if is_url_crawled(url):
            crawled_urls += 1
        else:
            target_article_urls.append(url)

    target_article_num = len(target_article_urls)

    # for url in target_article_urls:
    #     crawl_queue.put(url)

    log(f"ì´ {total_articles}ê°œì¤‘ ì´ë¯¸ ìˆ˜ì§‘ì™„ë£Œëœ {crawled_urls}ê°œëŠ” ì œì™¸í•©ë‹ˆë‹¤.")
    log(f"{len(target_article_urls)}ê°œì˜ ì‘ì—…ì„ {num_threads}ê°œì˜ ìŠ¤ë ˆë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    completed_tasks = 0
    success_count = 0
    failed_count = 0

    # target_article_urlsì„ (i % num_threads) + 1 ê°’ìœ¼ë¡œ ë¶„ë¥˜í•´ì„œ Listë¡œ ë§Œë“­ë‹ˆë‹¤.
    # ê° ì›Œì»¤ì— í• ë‹¹ë  URL ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    worker_urls = {i + 1: [] for i in range(num_threads)}

    # URLì„ ì›Œì»¤ IDì— ë”°ë¼ ë¶„ë°°
    for i, url in enumerate(target_article_urls):
        worker_id = (i % num_threads) + 1
        worker_urls[worker_id].append(url)

    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        future_to_url = {
            executor.submit(crawl_worker, worker_id, download_path, url_list_for_worker): url_list_for_worker
            for worker_id, url_list_for_worker in worker_urls.items()
        }

        for future in concurrent.futures.as_completed(future_to_url):
            # Signal workers to stop
            if stop_event.is_set():
                # ì¤‘ì§€
                break

            completed_tasks += 1
            try:
                # ì›Œì»¤ì˜ return ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                result_list = future.result()
                for result in result_list:
                    if result["status"] == "SUCCESS":
                        success_count += 1
                        log(f"[ì„±ê³µ] {result["message"]}")
                    elif result["status"] == "FAILED":
                        failed_count += 1
                        log(f"[ì‹¤íŒ¨] {result["message"]}")
                    elif result["status"] == "SKIPPED":
                        # ì´ ê²½ìš°ëŠ” ë¯¸ë¦¬ í•„í„°ë§í•´ì„œ ë°œìƒí•˜ì§€ ì•Šì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ ë‘¡ë‹ˆë‹¤.
                        log(f"[ê±´ë„ˆëœ€] {result["message"]}")

            except Exception as exc:
                # future.result() ìì²´ì—ì„œ ì˜ˆì™¸ê°€ ë°œìƒí•œ ê²½ìš° (ë§¤ìš° ë“œë¬¾)
                failed_count += 1
                log(f"[ì¹˜ëª…ì  ì˜¤ë¥˜] {future_to_url[future]}: {exc}")

            # ì§„í–‰ë¥ ì„ ì •í™•í•˜ê²Œ ì—…ë°ì´íŠ¸
            progress = (completed_tasks / target_article_num) * 100
            progress_bar['value'] = progress

    if not stop_event.is_set():
        progress_bar['value'] = 100
        log("\n\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        messagebox.showinfo("ì™„ë£Œ", "í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        log("í¬ë¡¤ë§ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        messagebox.showinfo("ì¤‘ì§€", "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    start_button.config(state=tk.NORMAL)
    stop_button.config(state=tk.DISABLED)
    progress_bar['value'] = 0


def start_crawling():
    global master_thread
    master_thread = threading.Thread(target=master_crawl_thread)
    master_thread.daemon = True
    master_thread.start()


def stop_crawling():
    if master_thread and master_thread.is_alive():
        log("í¬ë¡¤ë§ ì¤‘ì§€ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤...")
        stop_event.set()


def on_closing():
    if messagebox.askokcancel("ì¢…ë£Œ", "í¬ë¡¤ëŸ¬ë¥¼ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
        stop_event.set()
        if master_thread and master_thread.is_alive():
            master_thread.join()
        root.destroy()


# --- UI Setup ---
root = tk.Tk()
root.title("ë§ˆë‚˜í† ë¼ ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ëŸ¬")
root.geometry("700x500")

# --- Top Frame ---
top_frame = ttk.Frame(root)
top_frame.pack(pady=5, padx=10, fill='x', expand=False)

# URL Input
url_label = ttk.Label(top_frame, text="URL:")
url_label.pack(side='left', padx=(0, 5))
url_entry = ttk.Entry(top_frame)
url_entry.pack(side='left', fill='x', expand=True)

# --- Download Path Frame ---
path_frame = ttk.Frame(root)
path_frame.pack(pady=5, padx=10, fill='x', expand=False)

path_label = ttk.Label(path_frame, text="ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:")
path_label.pack(side='left', padx=(0, 5))
download_path_entry = ttk.Entry(path_frame)
download_path_entry.pack(side='left', fill='x', expand=True)
browse_button = ttk.Button(path_frame, text="ì°¾ì•„ë³´ê¸°...", command=browse_directory)
browse_button.pack(side='left', padx=(5, 0))

# --- Control Frame ---
control_frame = ttk.Frame(root)
control_frame.pack(pady=5, padx=10, fill='x', expand=False)

# URL Type
url_type_label = ttk.Label(control_frame, text="URL TYPE:")
url_type_label.pack(side='left', padx=(0, 5))
url_type = tk.StringVar(value="ëª©ë¡")
radio_single = ttk.Radiobutton(control_frame, text="ë‹¨í¸", variable=url_type, value="ë‹¨í¸", state=tk.DISABLED)
radio_single.pack(side='left', padx=5)
radio_list = ttk.Radiobutton(control_frame, text="ëª©ë¡", variable=url_type, value="ëª©ë¡")
radio_list.pack(side='left', padx=5)

# Spacer
spacer = ttk.Label(control_frame, text="")
spacer.pack(side='left', padx=10)

# Num Threads
num_threads_label = ttk.Label(control_frame, text="ìŠ¤ë ˆë“œ ê°œìˆ˜:")
num_threads_label.pack(side='left', padx=(0, 5))
num_threads_entry = ttk.Entry(control_frame, width=5)
num_threads_entry.pack(side='left')
num_threads_entry.insert(0, "3")  # Default value

# --- Button Frame ---
button_frame = ttk.Frame(root)
button_frame.pack(pady=5, padx=10, fill='x', expand=False)

start_button = ttk.Button(button_frame, text="ì‹œì‘", command=start_crawling)
start_button.pack(side='left', padx=5)
stop_button = ttk.Button(button_frame, text="ì¤‘ì§€", command=stop_crawling, state=tk.DISABLED)
stop_button.pack(side='left', padx=(0, 5))

# --- Log Frame ---
log_frame = ttk.Frame(root)
log_frame.pack(pady=10, padx=10, fill='both', expand=True)
log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, height=10)
log_text.pack(fill='both', expand=True)

# --- Progress Bar ---
progress_bar = ttk.Progressbar(root, orient='horizontal', length=100, mode='determinate')
progress_bar.pack(pady=10, padx=10, fill='x', expand=False)

root.protocol("WM_DELETE_WINDOW", on_closing)
root.mainloop()
