import concurrent.futures
import mimetypes
import os
import random
import threading
import time
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog

import requests
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from seleniumbase import Driver

from database import is_url_crawled, add_crawled_url

# --- Global Variables ---
master_thread = None
stop_event = threading.Event()


# crawl_queue = queue.Queue()

# --- Logging Function ---
def log(message):
    """GUIì˜ ë¡œê·¸ í…ìŠ¤íŠ¸ ì˜ì—­ì— ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

    ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ GUIë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ `update_idletasks`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        message (str): ë¡œê·¸ì— í‘œì‹œí•  ë©”ì‹œì§€.
    """
    log_text.insert(tk.END, message + "\n")
    log_text.see(tk.END)
    root.update_idletasks()



def create_text_file(download_path, content, file_name="list_url.txt"):
    """ì§€ì •ëœ ê²½ë¡œì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‚´ìš©ì„ ì”ë‹ˆë‹¤.

    ì£¼ë¡œ í¬ë¡¤ë§ ëŒ€ìƒì´ ëœ ëª©ë¡ í˜ì´ì§€ì˜ URLì„ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    Args:
        download_path (str): íŒŒì¼ì´ ìƒì„±ë  ë””ë ‰í„°ë¦¬ ê²½ë¡œ.
        content (str): íŒŒì¼ì— ì“¸ ë‚´ìš©.
        file_name (str): ìƒì„±í•  íŒŒì¼ì˜ ì´ë¦„.
    """
    abs_path = os.path.join(download_path, file_name)
    
    # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(abs_path):
        with open(abs_path, 'w') as file:
            file.write(content)


def browse_directory():
    """ë‹¤ìš´ë¡œë“œ ê²½ë¡œë¥¼ ì„ íƒí•˜ê¸° ìœ„í•œ íŒŒì¼ íƒìƒ‰ê¸° ëŒ€í™”ìƒìë¥¼ ì—½ë‹ˆë‹¤.

    ì‚¬ìš©ìê°€ ë””ë ‰í„°ë¦¬ë¥¼ ì„ íƒí•˜ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ GUIì˜ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì…ë ¥ í•„ë“œì—
    ìë™ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
    """
    path = filedialog.askdirectory()
    if path:
        download_path_entry.delete(0, tk.END)
        download_path_entry.insert(0, path)


# --- Crawler Functions ---
def scroll_to_bottom_with_pagedown(driver, max_scrolls=500, sleep_time=0.2):
    """Selenium ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ PAGE_DOWN í‚¤ë¥¼ ë³´ë‚´ í˜ì´ì§€ë¥¼ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤.

    ìŠ¤í¬ë¡¤ì´ ë” ì´ìƒ ì§„í–‰ë˜ì§€ ì•Šê±°ë‚˜ ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.

    Args:
        driver: Selenium ì›¹ ë“œë¼ì´ë²„ ì¸ìŠ¤í„´ìŠ¤.
        max_scrolls (int): ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜.
        sleep_time (float): ê° ìŠ¤í¬ë¡¤ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„(ì´ˆ).
    """
    # log("í˜ì´ì§€ì˜ ëê¹Œì§€ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    body = driver.find_element(By.TAG_NAME, "body")
    scroll_count = 0
    while scroll_count < max_scrolls and not stop_event.is_set():
        last_scroll_y = driver.execute_script("return window.scrollY")
        body.send_keys(Keys.PAGE_DOWN)
        scroll_count += 1
        time.sleep(sleep_time)
        new_scroll_y = driver.execute_script("return window.scrollY")
        if new_scroll_y == last_scroll_y:
            # log("í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ì— ë„ë‹¬í•˜ì—¬ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
    else:
        log(f"ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜({max_scrolls})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
    time.sleep(2)


def handle_captcha(driver, worker_id):
    """ìº¡ì±  í˜ì´ì§€ë¥¼ ê°ì§€í•˜ê³  ì‚¬ìš©ìê°€ ì§ì ‘ í•´ê²°í•  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.

    í˜„ì¬ URLì— 'bbs/captcha.php'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ìº¡ì±  í˜ì´ì§€ë¡œ ê°„ì£¼í•˜ê³ ,
    ì‚¬ìš©ìê°€ í•´ê²°í•˜ì—¬ URLì´ ë³€ê²½ë  ë•Œê¹Œì§€ 5ì´ˆ ê°„ê²©ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.

    Args:
        driver: Selenium ì›¹ ë“œë¼ì´ë²„ ì¸ìŠ¤í„´ìŠ¤.
        worker_id (int): í˜„ì¬ ì‘ì—…ì„ ìˆ˜í–‰ ì¤‘ì¸ ì›Œì»¤ì˜ ID.
    """
    while "bbs/captcha.php" in driver.current_url and not stop_event.is_set():
        log(f"ì›Œì»¤ {worker_id}: !! ìº¡ì±  í˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ !! ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í•´ê²°í•´ì£¼ì„¸ìš”.")
        while "bbs/captcha.php" in driver.current_url:
            time.sleep(5)
            if stop_event.is_set(): return
        log(f"ì›Œì»¤ {worker_id}: ìº¡ì± ê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        time.sleep(2)


def crawl_worker(worker_id, base_download_path, url_list):
    """ê°œë³„ í¬ë¡¤ëŸ¬ ìŠ¤ë ˆë“œê°€ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.

    ì£¼ì–´ì§„ URL ëª©ë¡ì„ ìˆœíšŒí•˜ë©° ê° í˜ì´ì§€ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    ìº¡ì±  ì²˜ë¦¬, ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°, ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.

    Args:
        worker_id (int): í˜„ì¬ ì‘ì—…ì„ ìˆ˜í–‰ ì¤‘ì¸ ì›Œì»¤ì˜ ID.
        base_download_path (str): ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê¸°ë³¸ ê²½ë¡œ.
        url_list (list): ì´ ì›Œì»¤ê°€ í¬ë¡¤ë§í•  URL ëª©ë¡.

    Returns:
        list: ê° URLì— ëŒ€í•œ í¬ë¡¤ë§ ê²°ê³¼(ì„±ê³µ, ì‹¤íŒ¨, ì¤‘ì§€ ë“±)ë¥¼ ë‹´ì€ ì‚¬ì „ ëª©ë¡.
    """
    result = []

    if not url_list:
        return result

    time.sleep(worker_id * 5)  # Stagger driver initialization

    driver = None
    try:

        driver = Driver(uc=True, headless=False)
        while not stop_event.is_set():
            for url in url_list:
                # url = crawl_queue.get(timeout=1)
                if len(url_list) == 0:
                    continue  # Check stop_event again

                try:
                    if is_url_crawled(url):
                        log(f"ì›Œì»¤ {worker_id}: ì´ë¯¸ í¬ë¡¤ë§ëœ URLì…ë‹ˆë‹¤: {url}")
                        continue

                    log(f"ì›Œì»¤ {worker_id}: Navigating to: {url}")
                    driver.get(url)

                    WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
                    )

                    handle_captcha(driver, worker_id)
                    if stop_event.is_set(): break

                    scroll_to_bottom_with_pagedown(driver)

                    page_source = driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')

                    title_element = soup.find('h1') or soup.find('div', class_='view-title')
                    if title_element:
                        post_title = title_element.get_text(strip=True)
                        if "ë§ˆë‚˜í† ë¼ -" in post_title:
                            post_title = post_title.replace(" > ë§ˆë‚˜í† ë¼ - ì¼ë³¸ë§Œí™” í—ˆë¸Œ", "").strip()
                    else:
                        post_title = f"untitled_post_{random.randint(1000, 9999)}"
                    log(f"ì›Œì»¤ {worker_id}: Post Title: {post_title}")

                    download_dir = os.path.join(base_download_path, post_title)
                    os.makedirs(download_dir, exist_ok=True)

                    html_mana_section = soup.find('section', itemtype='http://schema.org/NewsArticle')
                    if html_mana_section:
                        img_tags = html_mana_section.find_all('img')
                        log(f"ì›Œì»¤ {worker_id}: Found {len(img_tags)} images.")

                        for i, img in enumerate(img_tags):
                            if stop_event.is_set(): break
                            img_url = img.get('src')
                            if img_url and '.gif' not in img_url.lower():
                                try:
                                    header = {'referer': 'https://manatoki468.net/'}
                                    response = requests.get(img_url, stream=True, headers=header, timeout=30)
                                    response.raise_for_status()

                                    content_type = response.headers.get('Content-Type')
                                    ext = mimetypes.guess_extension(content_type) or os.path.splitext(img_url)[1] or ".jpg"
                                    img_filename = os.path.join(download_dir, f"{i + 1:03d}{ext}")
                                    with open(img_filename, 'wb') as f:
                                        f.write(response.content)
                                    # log(f"ì›Œì»¤ {worker_id}: Downloaded {img_filename}")
                                except requests.exceptions.RequestException as req_err:
                                    log(f"ì›Œì»¤ {worker_id}: Error downloading {img_url}: {req_err}")

                        if not stop_event.is_set():
                            add_crawled_url(url, post_title)
                            log(f"ì›Œì»¤ {worker_id}: [SUCCESS] í¬ë¡¤ë§ ì™„ë£Œ - {post_title}")
                            result.append({"state": "SUCCESS", "message": "ì„±ê³µ", "title": post_title, "url": url})
                    else:
                        log(f"ì›Œì»¤ {worker_id}: [FAIL] mana_sectionì´ ì—†ìŠµë‹ˆë‹¤. {url}")
                        result.append({"state": "STOPED", "message": "mana_sectionì´ ì—†ìŠµë‹ˆë‹¤.", "title": post_title, "url": url})

                except Exception as e:
                    log(f"ì›Œì»¤ {worker_id}: [FAIL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ {url}. {e}")
                    result.append({"state": "FAIL", "message": f"[FAIL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {e}", "title": "", "url": url})
            # future ë°˜í™˜
            return result
    finally:
        if driver:
            driver.quit()
        log(f"ì›Œì»¤ {worker_id}: ì¢…ë£Œ")


def get_target_pages(driver, target_url):
    """ë§Œí™” ëª©ë¡ í˜ì´ì§€ì—ì„œ ëª¨ë“  ê°œë³„ ì—í”¼ì†Œë“œ í˜ì´ì§€ì˜ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        driver: Selenium ì›¹ ë“œë¼ì´ë²„ ì¸ìŠ¤í„´ìŠ¤.
        target_url (str): í¬ë¡¤ë§í•  ë§Œí™”ì˜ ë©”ì¸ ëª©ë¡ í˜ì´ì§€ URL.

    Returns:
        list: ì¶”ì¶œëœ ëª¨ë“  ì—í”¼ì†Œë“œ URLì˜ ëª©ë¡. ì‹¤íŒ¨ ì‹œ ë¹ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        log(f"í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤: {target_url}")
        driver.get(target_url)
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
        )
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        article_body = soup.find('article', itemprop='articleBody')
        if article_body:
            serial_list_div = article_body.find('div', class_='serial-list')
            links = serial_list_div.find_all('a', href=True) if serial_list_div else []
            article_urls = [link['href'] for link in links]
            log(f"ì´ {len(article_urls)}ê°œì˜ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return article_urls
        else:
            log("ë§Œí™” ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
    except Exception as e:
        log(f"ëª©ë¡ í˜ì´ì§€ ë¡œë”© ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []


def master_crawl_thread():
    """í¬ë¡¤ë§ ì‘ì—…ì„ ì´ê´„í•˜ëŠ” ë©”ì¸ ìŠ¤ë ˆë“œ í•¨ìˆ˜ì…ë‹ˆë‹¤.

    GUIì—ì„œ ì…ë ¥ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬ë¡¤ë§ì„ ì„¤ì •í•˜ê³ , ì‘ì—…ì ìŠ¤ë ˆë“œë¥¼ ìƒì„±í•˜ì—¬
    URL ëª©ë¡ì„ ë¶„ë°°í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤. ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ê±°ë‚˜ ì¤‘ì§€ë  ë•Œê¹Œì§€
    ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    target_url = url_entry.get()
    if not target_url:
        messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    download_path = download_path_entry.get()
    if not download_path:
        download_path = "download_mana"
        log(f"ë‹¤ìš´ë¡œë“œ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ í´ë” '{download_path}'ì— ì €ì¥í•©ë‹ˆë‹¤.")
    os.makedirs(download_path, exist_ok=True)

    crawl_type = url_type.get()
    if crawl_type != "ëª©ë¡":
        messagebox.showinfo("ì•Œë¦¼", "í˜„ì¬ 'ëª©ë¡' ìœ í˜•ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
        return

    try:
        num_threads = int(num_threads_entry.get())
        if not 1 <= num_threads <= 10:
            raise ValueError()
    except ValueError:
        messagebox.showerror("ì˜¤ë¥˜", "ìŠ¤ë ˆë“œ ê°œìˆ˜ëŠ” 1ì—ì„œ 10 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return

    log("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    start_button.config(state=tk.DISABLED)
    stop_button.config(state=tk.NORMAL)
    progress_bar['value'] = 0
    stop_event.clear()

    # Clear queue from previous runs
    # while not crawl_queue.empty():
    #     crawl_queue.get_nowait()

    list_driver = None
    try:
        list_driver = Driver(uc=True, headless=False)
        article_urls = get_target_pages(list_driver, target_url)
    finally:
        if list_driver:
            list_driver.quit()

    if not article_urls:
        log("í¬ë¡¤ë§í•  ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        start_button.config(state=tk.NORMAL)
        stop_button.config(state=tk.DISABLED)
        return

    # target list urlì˜ txt íŒŒì¼ ìƒì„±. ì°¸ê³ ìš©
    create_text_file(download_path, target_url)

    # ìˆ˜ì§‘ëœ URL ì¤‘ ì´ë¯¸ ìˆ˜ì§‘ëœ URL ì œì™¸
    total_articles = len(article_urls)
    crawled_urls = 0
    target_article_urls = []

    for url in article_urls:
        if is_url_crawled(url):
            crawled_urls += 1
        else:
            target_article_urls.append(url)

    target_article_num = len(target_article_urls)




    log(f"ì´ {total_articles}ê°œì¤‘ ì´ë¯¸ ìˆ˜ì§‘ì™„ë£Œëœ {crawled_urls}ê°œëŠ” ì œì™¸í•©ë‹ˆë‹¤.")

    if target_article_num <= 0:
        log("í¬ë¡¤ë§í•  ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        start_button.config(state=tk.NORMAL)
        stop_button.config(state=tk.DISABLED)
        return

    log(f"{len(target_article_urls)}ê°œì˜ ì‘ì—…ì„ {num_threads}ê°œì˜ ìŠ¤ë ˆë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    completed_tasks = 0
    success_count = 0
    failed_count = 0

    # target_article_urlsì„ (i % num_threads) + 1 ê°’ìœ¼ë¡œ ë¶„ë¥˜í•´ì„œ Listë¡œ ë§Œë“­ë‹ˆë‹¤.
    # ê° ì›Œì»¤ì— í• ë‹¹ë  URL ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    worker_urls = {i + 1: [] for i in range(num_threads)}

    # URLì„ ì›Œì»¤ IDì— ë”°ë¼ ë¶„ë°°
    for i, url in enumerate(target_article_urls):
        worker_id = (i % num_threads) + 1
        worker_urls[worker_id].append(url)

    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        future_to_url = {
            executor.submit(crawl_worker, worker_id, download_path, url_list_for_worker): url_list_for_worker
            for worker_id, url_list_for_worker in worker_urls.items()
        }

        for future in concurrent.futures.as_completed(future_to_url):
            # Signal workers to stop
            if stop_event.is_set():
                # ì¤‘ì§€
                break

            completed_tasks += 1
            try:
                # ì›Œì»¤ì˜ return ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                for result in future.result():
                    if result["state"] == "SUCCESS":
                        success_count += 1
                    elif result["status"] == "FAILED":
                        failed_count += 1
                    elif result["status"] == "SKIPPED":
                        # ì´ ê²½ìš°ëŠ” ë¯¸ë¦¬ í•„í„°ë§í•´ì„œ ë°œìƒí•˜ì§€ ì•Šì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ ë‘¡ë‹ˆë‹¤.
                        log(f"[ê±´ë„ˆëœ€] {result["message"]}")

                log(f"ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failed_count}")

            except Exception as exc:
                # future.result() ìì²´ì—ì„œ ì˜ˆì™¸ê°€ ë°œìƒí•œ ê²½ìš° (ë§¤ìš° ë“œë¬¾)
                failed_count += 1
                log(f"[ì¹˜ëª…ì  ì˜¤ë¥˜] {future_to_url[future]}: {exc}")

            # ì§„í–‰ë¥ ì„ ì •í™•í•˜ê²Œ ì—…ë°ì´íŠ¸
            if not target_article_num == 0:
                progress = (completed_tasks / target_article_num) * 100
                progress_bar['value'] = progress
            else:
                progress_bar['value'] = 100

    if not stop_event.is_set():
        progress_bar['value'] = 100
        log("\n\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        messagebox.showinfo("ì™„ë£Œ", "í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        log("í¬ë¡¤ë§ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        messagebox.showinfo("ì¤‘ì§€", "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    start_button.config(state=tk.NORMAL)
    stop_button.config(state=tk.DISABLED)



def start_crawling():
    """'ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    `master_crawl_thread`ë¥¼ ë³„ë„ì˜ ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ìƒì„±í•˜ê³  ì‹œì‘í•˜ì—¬
    GUIê°€ ë©ˆì¶”ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
    """
    global master_thread
    master_thread = threading.Thread(target=master_crawl_thread)
    master_thread.daemon = True
    master_thread.start()


def stop_crawling():
    """'ì¤‘ì§€' ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    `stop_event`ë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë“  í™œì„± ìŠ¤ë ˆë“œ(ë§ˆìŠ¤í„° ë° ì›Œì»¤)ì—
    ì¤‘ì§€ ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
    """
    if master_thread and master_thread.is_alive():
        log("í¬ë¡¤ë§ ì¤‘ì§€ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤...")
        stop_event.set()


def on_closing():
    """GUI ì°½ì„ ë‹«ì„ ë•Œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    ì‚¬ìš©ìì—ê²Œ ì¢…ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ , ì§„í–‰ ì¤‘ì¸ í¬ë¡¤ë§ ìŠ¤ë ˆë“œê°€
    ì•ˆì „í•˜ê²Œ ì¢…ë£Œë  ìˆ˜ ìˆë„ë¡ `stop_event`ë¥¼ ì„¤ì •í•œ í›„ ì°½ì„ ë‹«ìŠµë‹ˆë‹¤.
    """
    if messagebox.askokcancel("ì¢…ë£Œ", "í¬ë¡¤ëŸ¬ë¥¼ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
        stop_event.set()
        if master_thread and master_thread.is_alive():
            master_thread.join()
        root.destroy()



# --- UI Setup ---
root = tk.Tk()
root.title("ë§ˆë‚˜í† ë¼ ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ëŸ¬")
root.geometry("700x500")

# --- Top Frame ---
top_frame = ttk.Frame(root)
top_frame.pack(pady=5, padx=10, fill='x', expand=False)

# URL Input
url_label = ttk.Label(top_frame, text="URL:")
url_label.pack(side='left', padx=(0, 5))
url_entry = ttk.Entry(top_frame)
url_entry.pack(side='left', fill='x', expand=True)

# --- Download Path Frame ---
path_frame = ttk.Frame(root)
path_frame.pack(pady=5, padx=10, fill='x', expand=False)

path_label = ttk.Label(path_frame, text="ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:")
path_label.pack(side='left', padx=(0, 5))
download_path_entry = ttk.Entry(path_frame)
download_path_entry.pack(side='left', fill='x', expand=True)
browse_button = ttk.Button(path_frame, text="ì°¾ì•„ë³´ê¸°...", command=browse_directory)
browse_button.pack(side='left', padx=(5, 0))

# --- Control Frame ---
control_frame = ttk.Frame(root)
control_frame.pack(pady=5, padx=10, fill='x', expand=False)

# URL Type
url_type_label = ttk.Label(control_frame, text="URL TYPE:")
url_type_label.pack(side='left', padx=(0, 5))
url_type = tk.StringVar(value="ëª©ë¡")
radio_single = ttk.Radiobutton(control_frame, text="ë‹¨í¸", variable=url_type, value="ë‹¨í¸", state=tk.DISABLED)
radio_single.pack(side='left', padx=5)
radio_list = ttk.Radiobutton(control_frame, text="ëª©ë¡", variable=url_type, value="ëª©ë¡")
radio_list.pack(side='left', padx=5)

# Spacer
spacer = ttk.Label(control_frame, text="")
spacer.pack(side='left', padx=10)

# Num Threads
num_threads_label = ttk.Label(control_frame, text="ìŠ¤ë ˆë“œ ê°œìˆ˜:")
num_threads_label.pack(side='left', padx=(0, 5))
num_threads_entry = ttk.Entry(control_frame, width=5)
num_threads_entry.pack(side='left')
num_threads_entry.insert(0, "3")  # Default value

# --- Button Frame ---
button_frame = ttk.Frame(root)
button_frame.pack(pady=5, padx=10, fill='x', expand=False)

start_button = ttk.Button(button_frame, text="ì‹œì‘", command=start_crawling)
start_button.pack(side='left', padx=5)
stop_button = ttk.Button(button_frame, text="ì¤‘ì§€", command=stop_crawling, state=tk.DISABLED)
stop_button.pack(side='left', padx=(0, 5))

# --- Log Frame ---
log_frame = ttk.Frame(root)
log_frame.pack(pady=10, padx=10, fill='both', expand=True)
log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, height=10)
log_text.pack(fill='both', expand=True)

# --- Progress Bar ---
progress_bar = ttk.Progressbar(root, orient='horizontal', length=100, mode='determinate')
progress_bar.pack(pady=10, padx=10, fill='x', expand=False)

root.protocol("WM_DELETE_WINDOW", on_closing)
root.mainloop()
