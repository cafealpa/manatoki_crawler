import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import queue
import concurrent.futures
import mimetypes
import os
import random
import time
import requests
import sys
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from seleniumbase import Driver
from database import is_url_crawled, add_crawled_url

# --- Global Variables ---
master_thread = None
stop_event = threading.Event()
crawl_queue = queue.Queue()

# --- Logging Function ---
def log(message):
    """Inserts a message into the log text area in a thread-safe way."""
    log_text.insert(tk.END, message + "\n")
    log_text.see(tk.END)
    root.update_idletasks()

# --- Crawler Functions ---
def scroll_to_bottom_with_pagedown(driver, max_scrolls=50, sleep_time=0.2):
    # log("í˜ì´ì§€ì˜ ëê¹Œì§€ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    body = driver.find_element(By.TAG_NAME, "body")
    scroll_count = 0
    while scroll_count < max_scrolls and not stop_event.is_set():
        last_scroll_y = driver.execute_script("return window.scrollY")
        body.send_keys(Keys.PAGE_DOWN)
        scroll_count += 1
        time.sleep(sleep_time)
        new_scroll_y = driver.execute_script("return window.scrollY")
        if new_scroll_y == last_scroll_y:
            # log("í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ì— ë„ë‹¬í•˜ì—¬ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
    else:
        log(f"ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜({max_scrolls})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
    time.sleep(2)

def handle_captcha(driver, worker_id):
    while "bbs/captcha.php" in driver.current_url and not stop_event.is_set():
        log(f"ì›Œì»¤ {worker_id}: !! ìº¡ì±  í˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ !! ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í•´ê²°í•´ì£¼ì„¸ìš”.")
        while "bbs/captcha.php" in driver.current_url:
            time.sleep(5)
            if stop_event.is_set(): return
        log(f"ì›Œì»¤ {worker_id}: ìº¡ì± ê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        time.sleep(2)

def crawl_worker(worker_id):
    """The function each thread will execute to crawl pages."""
    time.sleep(worker_id * 2) # Stagger driver initialization
    driver = None
    try:
        driver = Driver(uc=True, headless=False)
        while not stop_event.is_set():
            try:
                url = crawl_queue.get(timeout=1)
                if url is None: # Poison pill
                    break
            except queue.Empty:
                continue # Check stop_event again

            try:
                if is_url_crawled(url):
                    log(f"ì›Œì»¤ {worker_id}: ì´ë¯¸ í¬ë¡¤ë§ëœ URLì…ë‹ˆë‹¤: {url}")
                    continue

                log(f"ì›Œì»¤ {worker_id}: Navigating to: {url}")
                driver.get(url)

                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
                )

                handle_captcha(driver, worker_id)
                if stop_event.is_set(): break

                scroll_to_bottom_with_pagedown(driver)

                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')

                title_element = soup.find('h1') or soup.find('div', class_='view-title')
                if title_element:
                    post_title = title_element.get_text(strip=True)
                    if "ë§ˆë‚˜í† ë¼ -" in post_title:
                        post_title = post_title.replace(" > ë§ˆë‚˜í† ë¼ - ì¼ë³¸ë§Œí™” í—ˆë¸Œ", "").strip()
                else:
                    post_title = f"untitled_post_{random.randint(1000, 9999)}"
                log(f"ì›Œì»¤ {worker_id}: Post Title: {post_title}")

                download_dir = os.path.join("download_mana", post_title)
                os.makedirs(download_dir, exist_ok=True)

                html_mana_section = soup.find('section', itemtype='http://schema.org/NewsArticle')
                if html_mana_section:
                    img_tags = html_mana_section.find_all('img')
                    log(f"ì›Œì»¤ {worker_id}: Found {len(img_tags)} images.")

                    for i, img in enumerate(img_tags):
                        if stop_event.is_set(): break
                        img_url = img.get('src')
                        if img_url and '.gif' not in img_url.lower():
                            try:
                                header = {'referer': 'https://manatoki468.net/'}
                                response = requests.get(img_url, stream=True, headers=header, timeout=30)
                                response.raise_for_status()

                                content_type = response.headers.get('Content-Type')
                                ext = mimetypes.guess_extension(content_type) or os.path.splitext(img_url)[1] or ".jpg"
                                img_filename = os.path.join(download_dir, f"{i + 1:03d}{ext}")
                                with open(img_filename, 'wb') as f:
                                    f.write(response.content)
                                # log(f"ì›Œì»¤ {worker_id}: Downloaded {img_filename}")
                            except requests.exceptions.RequestException as req_err:
                                log(f"ì›Œì»¤ {worker_id}: Error downloading {img_url}: {req_err}")
                    
                    if not stop_event.is_set():
                        add_crawled_url(url, post_title)
                        log(f"ì›Œì»¤ {worker_id}: [SUCCESS] í¬ë¡¤ë§ ì™„ë£Œ - {post_title}")
                else:
                    log(f"ì›Œì»¤ {worker_id}: [FAIL] mana_sectionì´ ì—†ìŠµë‹ˆë‹¤. {url}")

            except Exception as e:
                log(f"ì›Œì»¤ {worker_id}: [FAIL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ {url}. {e}")
            finally:
                crawl_queue.task_done()
    finally:
        if driver:
            driver.quit()
        log(f"ì›Œì»¤ {worker_id}: ì¢…ë£Œ")


def get_target_pages(driver, target_url):
    try:
        log(f"í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤: {target_url}")
        driver.get(target_url)
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
        )
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        article_body = soup.find('article', itemprop='articleBody')
        if article_body:
            serial_list_div = article_body.find('div', class_='serial-list')
            links = serial_list_div.find_all('a', href=True) if serial_list_div else []
            article_urls = [link['href'] for link in links]
            log(f"ì´ {len(article_urls)}ê°œì˜ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return article_urls
        else:
            log("ë§Œí™” ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
    except Exception as e:
        log(f"ëª©ë¡ í˜ì´ì§€ ë¡œë”© ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []

def master_crawl_thread():
    target_url = url_entry.get()
    if not target_url:
        messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    crawl_type = url_type.get()
    if crawl_type != "ëª©ë¡":
        messagebox.showinfo("ì•Œë¦¼", "í˜„ì¬ 'ëª©ë¡' ìœ í˜•ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
        return
        
    try:
        num_threads = int(num_threads_entry.get())
        if not 1 <= num_threads <= 10:
            raise ValueError()
    except ValueError:
        messagebox.showerror("ì˜¤ë¥˜", "ìŠ¤ë ˆë“œ ê°œìˆ˜ëŠ” 1ì—ì„œ 10 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return

    log("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    start_button.config(state=tk.DISABLED)
    stop_button.config(state=tk.NORMAL)
    progress_bar['value'] = 0
    stop_event.clear()

    # Clear queue from previous runs
    while not crawl_queue.empty():
        crawl_queue.get_nowait()

    list_driver = None
    try:
        list_driver = Driver(uc=True, headless=False)
        article_urls = get_target_pages(list_driver, target_url)
    finally:
        if list_driver:
            list_driver.quit()

    if not article_urls:
        log("í¬ë¡¤ë§í•  ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        start_button.config(state=tk.NORMAL)
        stop_button.config(state=tk.DISABLED)
        return

    total_articles = len(article_urls)
    for url in article_urls:
        crawl_queue.put(url)

    log(f"{crawl_queue.qsize()}ê°œì˜ ì‘ì—…ì„ {num_threads}ê°œì˜ ìŠ¤ë ˆë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(crawl_worker, i + 1) for i in range(num_threads)]

        while not crawl_queue.empty() and not stop_event.is_set():
            progress = (total_articles - crawl_queue.qsize()) / total_articles * 100
            progress_bar['value'] = progress
            time.sleep(1)
        
        # Signal workers to stop
        if stop_event.is_set():
            # Clear the queue and send poison pills
            while not crawl_queue.empty():
                crawl_queue.get_nowait()
            for _ in range(num_threads):
                crawl_queue.put(None)
        else: # Normal completion
             crawl_queue.join()


    if not stop_event.is_set():
        progress_bar['value'] = 100
        log("\n\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        messagebox.showinfo("ì™„ë£Œ", "í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        log("í¬ë¡¤ë§ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        messagebox.showinfo("ì¤‘ì§€", "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")


    start_button.config(state=tk.NORMAL)
    stop_button.config(state=tk.DISABLED)
    progress_bar['value'] = 0


def start_crawling():
    global master_thread
    master_thread = threading.Thread(target=master_crawl_thread)
    master_thread.daemon = True
    master_thread.start()

def stop_crawling():
    if master_thread and master_thread.is_alive():
        log("í¬ë¡¤ë§ ì¤‘ì§€ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤...")
        stop_event.set()

def on_closing():
    if messagebox.askokcancel("ì¢…ë£Œ", "í¬ë¡¤ëŸ¬ë¥¼ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
        stop_event.set()
        if master_thread and master_thread.is_alive():
            master_thread.join()
        root.destroy()


# --- UI Setup ---
root = tk.Tk()
root.title("ë§ˆë‚˜í† ë¼ ë©€í‹°ìŠ¤ë ˆë“œ í¬ë¡¤ëŸ¬")
root.geometry("700x500")

# --- Top Frame ---
top_frame = ttk.Frame(root)
top_frame.pack(pady=5, padx=10, fill='x', expand=False)

# URL Input
url_label = ttk.Label(top_frame, text="URL:")
url_label.pack(side='left', padx=(0, 5))
url_entry = ttk.Entry(top_frame)
url_entry.pack(side='left', fill='x', expand=True)

# --- Control Frame ---
control_frame = ttk.Frame(root)
control_frame.pack(pady=5, padx=10, fill='x', expand=False)

# URL Type
url_type_label = ttk.Label(control_frame, text="URL TYPE:")
url_type_label.pack(side='left', padx=(0, 5))
url_type = tk.StringVar(value="ëª©ë¡")
radio_single = ttk.Radiobutton(control_frame, text="ë‹¨í¸", variable=url_type, value="ë‹¨í¸", state=tk.DISABLED)
radio_single.pack(side='left', padx=5)
radio_list = ttk.Radiobutton(control_frame, text="ëª©ë¡", variable=url_type, value="ëª©ë¡")
radio_list.pack(side='left', padx=5)

# Spacer
spacer = ttk.Label(control_frame, text="")
spacer.pack(side='left', padx=10)

# Num Threads
num_threads_label = ttk.Label(control_frame, text="ìŠ¤ë ˆë“œ ê°œìˆ˜:")
num_threads_label.pack(side='left', padx=(0, 5))
num_threads_entry = ttk.Entry(control_frame, width=5)
num_threads_entry.pack(side='left')
num_threads_entry.insert(0, "3") # Default value

# --- Button Frame ---
button_frame = ttk.Frame(root)
button_frame.pack(pady=5, padx=10, fill='x', expand=False)

start_button = ttk.Button(button_frame, text="ì‹œì‘", command=start_crawling)
start_button.pack(side='left', padx=5)
stop_button = ttk.Button(button_frame, text="ì¤‘ì§€", command=stop_crawling, state=tk.DISABLED)
stop_button.pack(side='left', padx=(0, 5))

# --- Log Frame ---
log_frame = ttk.Frame(root)
log_frame.pack(pady=10, padx=10, fill='both', expand=True)
log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, height=10)
log_text.pack(fill='both', expand=True)

# --- Progress Bar ---
progress_bar = ttk.Progressbar(root, orient='horizontal', length=100, mode='determinate')
progress_bar.pack(pady=10, padx=10, fill='x', expand=False)

root.protocol("WM_DELETE_WINDOW", on_closing)
root.mainloop()