import mimetypes
import os
import random
import time

import requests
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from seleniumbase import Driver

from database import is_url_crawled, add_crawled_url


def scroll_to_bottom_with_pagedown(driver, max_scrolls=50, sleep_time=0.2):
    """
    PAGE_DOWN í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ê¹Œì§€ ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤.
    ìŠ¤í¬ë¡¤ ìœ„ì¹˜ê°€ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

    Args:
        driver: Selenium ë“œë¼ì´ë²„ ê°ì²´.
        max_scrolls (int): ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜.
        sleep_time (float): ê° ìŠ¤í¬ë¡¤ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ (ì´ˆ).
    """
    print("í˜ì´ì§€ì˜ ëê¹Œì§€ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    body = driver.find_element(By.TAG_NAME, "body")
    scroll_count = 0

    while scroll_count < max_scrolls:
        # ìŠ¤í¬ë¡¤ ì „ì˜ ìˆ˜ì§ ìŠ¤í¬ë¡¤ ìœ„ì¹˜ë¥¼ ê¸°ë¡
        last_scroll_y = driver.execute_script("return window.scrollY")

        # í˜ì´ì§€ ë‹¤ìš´ í‚¤ ì…ë ¥
        body.send_keys(Keys.PAGE_DOWN)
        scroll_count += 1

        # ìƒˆ ì½˜í…ì¸ ê°€ ë¡œë“œë  ì‹œê°„ì„ ì¤Œ
        time.sleep(sleep_time)

        # ìŠ¤í¬ë¡¤ í›„ì˜ ìˆ˜ì§ ìŠ¤í¬ë¡¤ ìœ„ì¹˜ë¥¼ í™•ì¸
        new_scroll_y = driver.execute_script("return window.scrollY")

        # ìŠ¤í¬ë¡¤ ìœ„ì¹˜ì— ë³€í™”ê°€ ì—†ë‹¤ë©´, í˜ì´ì§€ì˜ ëì— ë„ë‹¬í•œ ê²ƒì„
        if new_scroll_y == last_scroll_y:
            print("í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ì— ë„ë‹¬í•˜ì—¬ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
    else:
        # while ë£¨í”„ê°€ break ì—†ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆì„ ë•Œ (ìµœëŒ€ íšŸìˆ˜ ë„ë‹¬)
        print(f"ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜({max_scrolls})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")

    # ëª¨ë“  ì½˜í…ì¸ ê°€ í™•ì‹¤íˆ ë¡œë“œë˜ë„ë¡ ë§ˆì§€ë§‰ì— ì¶”ê°€ ëŒ€ê¸°
    time.sleep(2)

def handle_capcha(driver):
    while "bbs/captcha.php" in driver.current_url:
        print("\n" + "=" * 50)
        print("!! ìº¡ì±  í˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ !!")
        print("ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ìº¡ì± ë¥¼ í’€ì–´ì£¼ì„¸ìš”.")
        input("ì™„ë£Œí•˜ê³  í„°ë¯¸ë„ì—ì„œ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.......")
        print("=" * 50 + "\n")
        time.sleep(2)

    return


def crawl_mana_page(driver, article_urls):
    for url in article_urls:
        # ê¸°ì¡´ í¬ë¡¤ë§ ì—¬ë¶€ í™•ì¸
        if is_url_crawled(url):
            print(f"ì´ë¯¸ í¬ë¡¤ë§ëœ URLì…ë‹ˆë‹¤: {url}")
            continue

        # í˜ì´ì§€ ì´ë™
        print(f"\nNavigating to: {url}")
        driver.get(url)

        # ìº¡ì±  í˜ì´ì§€
        handle_capcha(driver)

        # í˜ì´ì§€ì˜ body ìš”ì†Œë¥¼ ì„ íƒ
        body = driver.find_element(By.TAG_NAME, "body")

        # ë°©ë²• A: Page Down í‚¤ë¥¼ ëˆŒëŸ¬ í•œ í™”ë©´ì”© ìŠ¤í¬ë¡¤
        # for _ in range(20):
        #     body.send_keys(Keys.PAGE_DOWN)
        #     time.sleep(0.1)
        #
        # time.sleep(2)
        #
        # for _ in range(10):
        #     body.send_keys(Keys.PAGE_DOWN)
        # time.sleep(0.1)

        # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
        scroll_to_bottom_with_pagedown(driver)

        # í˜ì´ì§€ ë¡œë”© í›„ ì´ë¯¸ì§€ ë¹„ë™ê¸° ë¡œë”© 1ì´ˆ ëŒ€ê¸°
        # time.sleep(1)

        # í˜ì´ì§€ ì†ŒìŠ¤ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # ê²Œì‹œë¬¼ ì œëª© ì¶”ì¶œ (í´ë” ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©)
        # Manatoki í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ h1 ë˜ëŠ” ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ divì—ì„œ ì œëª©ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” h1 íƒœê·¸ë¥¼ ë¨¼ì € ì‹œë„í•˜ê³ , ì—†ìœ¼ë©´ view-title í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
        title_element = soup.find('h1')
        if not title_element:
            title_element = soup.find('div', class_='view-title')  # ì˜ˆì‹œ: ì‹¤ì œ í´ë˜ìŠ¤ëª… í™•ì¸ í•„ìš”

        if title_element:
            post_title = title_element.get_text(strip=True)
            # íŒŒì¼ ì‹œìŠ¤í…œì— ì•ˆì „í•œ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
            post_title = "".join(c for c in post_title if c.isalnum() or c in (' ', '.', '_')).rstrip()

            # í˜ì´ì§€ titleì—ì„œ "ë§ˆë‚˜í† ë¼  ì¼ë³¸ë§Œí™” í—ˆë¸Œ"ì œê±°
            if "ë§ˆë‚˜í† ë¼  ì¼ë³¸ë§Œí™” í—ˆë¸Œ" in post_title:
                post_title = post_title.replace("ë§ˆë‚˜í† ë¼  ì¼ë³¸ë§Œí™” í—ˆë¸Œ", "").strip()

            print(f"Post Title: {post_title}")
        else:
            post_title = "untitled_post"
            print("Could not find post title. Using default name.")

        # ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
        if not os.path.exists("download_mana"):
            os.makedirs("download_mana")

        if not os.path.exists(os.path.join("download_mana", post_title)):
            os.makedirs(os.path.join("download_mana", post_title))
            print(f"Created directory: {os.path.join('download_mana', post_title)}")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        html_mana_section = soup.find('section', itemtype='http://schema.org/NewsArticle')
        if html_mana_section:
            img_tags = html_mana_section.find_all('img')
            print(f"Found {len(img_tags)} images.")

            for i, img in enumerate(img_tags):
                img_url = img.get('src')
                if img_url:

                    # gif í™•ì¥ìê°€ urlì— ìˆìœ¼ë©´ skip
                    if '.gif' in img_url.lower():
                        # print(f"Skipping GIF image: {img_url}")
                        continue

                    print(f"img url:{img_url}")

                    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    try:
                        header = {'referer': 'https://manatoki468.net/'}
                        response = requests.get(img_url, stream=True, headers=header)
                        response.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ

                        # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                        content_type = response.headers.get('Content-Type')
                        if content_type:
                            ext = mimetypes.guess_extension(content_type)
                            if not ext:  # guess_extensionì´ ì‹¤íŒ¨í•  ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
                                ext = os.path.splitext(img_url)[1]
                        else:
                            ext = os.path.splitext(img_url)[1]
                        if not ext:  # í™•ì¥ìê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                            ext = ".jpg"

                        img_filename = os.path.join("download_mana", post_title, f"{i + 1}{ext}")
                        with open(img_filename, 'wb') as f:
                            for chunk in response.iter_content(1024):
                                f.write(chunk)
                        print(f"Downloaded {img_filename}")

                    except requests.exceptions.RequestException as req_err:
                        print(f"Error downloading {img_url}: {req_err}")
                    except Exception as err:
                        print(f"An unexpected error occurred while downloading {img_url}: {err}")

                else:
                    print(f"img_url is None: {img_url}")

            # í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆì„ ê²½ìš°ë§Œ DBì— url ì¶”ê°€
            add_crawled_url(url, post_title)
        else:
            print("html_mana_section not found.")

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•˜ê¸° ì „ì— ëœë¤ ëŒ€ê¸° (1ì´ˆ ~ 9ì´ˆ)
        wait_time = random.randint(1, 5)
        print(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•˜ê¸° ì „ {wait_time}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
        time.sleep(wait_time)


# ëª©ë¡ ì¡°íšŒ
def get_target_pages(driver: Driver, target_url) -> list:
    """
        ì§€ì •ëœ ì›¹ í˜ì´ì§€ì—ì„œ ëŒ€ìƒ ê¸°ì‚¬ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ì´ ê¸°ëŠ¥ì€ ì œê³µëœ ì›¹ ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ `TARGET_URL`ë¡œ ì´ë™í•œ í›„
        ëŒ€ìƒ ì›¹ í˜ì´ì§€ì˜ ë³¸ë¬¸ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ” HTML ê¸°ì‚¬ ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.
        ë¡œë“œê°€ ì™„ë£Œë˜ë©´ ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•˜ì—¬ ì§€ì •ëœ â€˜serial-listâ€™ ì„¹ì…˜ì— í¬í•¨ëœ ì—°ì† URL ëª©ë¡ì„
        ì¶”ì¶œí•©ë‹ˆë‹¤.

        :param driver: ì›¹ í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤.
        :type driver: Driver
        :return: ê¸°ì‚¬ ë³¸ë¬¸ì˜ â€˜serial-listâ€™ ì„¹ì…˜ì— í¬í•¨ëœ URL ëª©ë¡ ë˜ëŠ” í•´ë‹¹ URLì´ ë°œê²¬ë˜ì§€ ì•Šì€ ê²½ìš° ë¹ˆ ëª©ë¡.
        :rtype: list
    """
    try:
        driver.get(target_url)
        print(f"Successfully opened {target_url}")

        # <article itemprop="articleBody"> ìš”ì†Œê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        # print("Waiting for <article itemprop='articleBody'> to load...")
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
        )
        # print("<article itemprop='articleBody'> loaded.")

        # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # articleBody ë‚´ì˜ ì—°ì¬ ëª©ë¡ URL ìˆ˜ì§‘
        article_body = soup.find('article', itemprop='articleBody')
        if article_body:
            # print("Found articleBody. Extracting URLs...")
            serial_list_div = article_body.find('div', class_='serial-list')
            if serial_list_div:
                links = serial_list_div.find_all('a', href=True)
            else:
                links = []
            article_urls = [link['href'] for link in links]

            print("Found articleBody. Extracted URLs:")
            for url in article_urls:
                print(url)

            return article_urls
        else:
            print("ë§Œí™” ëª©ë¡ì´ ì—†ë„¤ìš”.")
            return []
    except Exception as e:
        print(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    return []


def crawl_manatoki():
    # ë‹¤ìš´ë¡œë“œ í´ë” ìƒì„±
    if not os.path.exists("download_mana"):
        os.makedirs("download_mana")
        print("Created download_mana directory")

    url_input_flag = True
    target_url = None

    while url_input_flag:
        target_url = input("í¬ë¡¤ë§ í•  ë§Œí™” ëª©ë¡ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")

        if not target_url:
            print("URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        confirm_val = input(f"{target_url} ë§ë‚˜ìš”?(Y/n)")
        if confirm_val.lower() in ["", "ì˜ˆ", "y"]:
            url_input_flag = False

        continue

    print("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    driver = Driver(uc=True)  # uc=True for undetected_chromedriver
    try:
        # ëª©ë¡ ì¡°íšŒ
        article_target_list = get_target_pages(driver, target_url)
        # ì¡°íšŒëœ ëª©ë¡ìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        crawl_mana_page(driver, article_target_list)
        print("\n\nğŸ‰ğŸ‰ğŸ‰ í¬ë¡¤ë§ì´ ì™„ë£Œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    finally:
        driver.quit()


if __name__ == "__main__":
    crawl_manatoki()
