

import concurrent.futures
import mimetypes
import os
import random
import queue
import time
import sys

import requests
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from seleniumbase import Driver

from database import is_url_crawled, add_crawled_url

# ìŠ¤ë ˆë“œ 4ê°œë¶€í„° IP block ë¨.
NUM_THREADS = 3

def scroll_to_bottom_with_pagedown(driver, max_scrolls=80, sleep_time=0.2):
    print("í˜ì´ì§€ì˜ ëê¹Œì§€ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    body = driver.find_element(By.TAG_NAME, "body")
    scroll_count = 0
    last_scroll_y = driver.execute_script("return window.scrollY")

    while scroll_count < max_scrolls:
        # print(f"í˜„ì¬ ìŠ¤í¬ë¡¤ ìœ„ì¹˜: {last_scroll_y}")
        body.send_keys(Keys.PAGE_DOWN)
        time.sleep(sleep_time)

        current_scroll_y = driver.execute_script("return window.scrollY")
        # print(f"ìŠ¤í¬ë¡¤ í›„ ìœ„ì¹˜: {current_scroll_y}")

        if current_scroll_y == last_scroll_y:
            print("í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ì— ë„ë‹¬í•˜ì—¬ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        last_scroll_y = current_scroll_y
        scroll_count += 1
    else:
        print(f"ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜({max_scrolls})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
    
    time.sleep(1)


def handle_captcha(driver):
    while "bbs/captcha.php" in driver.current_url:
        print("\n" + "=" * 50)
        print("!! ìº¡ì±  í˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ !!")
        print("ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ìº¡ì± ë¥¼ í’€ì–´ì£¼ì„¸ìš”.")
        input("ì™„ë£Œí•˜ê³  í„°ë¯¸ë„ì—ì„œ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.......")
        print("=" * 50 + "\n")
        time.sleep(2)


def crawl_worker(q, worker_id):
    # ì›Œì»¤ id(1,2,3) * 5 ë§Œí¼ ê¸°ë‹¤ë¦¼. driver ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°
    time.sleep(worker_id * 5)

    driver = None
    try:

        driver = Driver(uc=True, headless=False)

        while True:
            url = q.get()

            # íì—ì„œ ë…ì•½ì„ ë°›ìœ¼ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œí•˜ê³  ìŠ¤ë ˆë“œë¥¼ ë§ˆì¹¨
            if url is None:
                q.task_done()
                break

            try:
                if is_url_crawled(url):
                    print(f"ì›Œì»¤ {worker_id}: ì´ë¯¸ í¬ë¡¤ë§ëœ URLì…ë‹ˆë‹¤: {url}")
                    continue

                print(f"ì›Œì»¤ {worker_id}: Navigating to: {url}")
                driver.get(url)

                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
                )

                handle_captcha(driver)
                scroll_to_bottom_with_pagedown(driver)

                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')

                title_element = soup.find('h1')
                if not title_element:
                    title_element = soup.find('div', class_='view-title')

                if title_element:
                    post_title = title_element.get_text(strip=True)
                    if "ë§ˆë‚˜í† ë¼ -" in post_title:
                        post_title = post_title.replace(" > ë§ˆë‚˜í† ë¼ - ì¼ë³¸ë§Œí™” í—ˆë¸Œ", "").strip()
                    print(f"ì›Œì»¤ {worker_id}: Post Title: {post_title}")
                else:
                    post_title = f"untitled_post_{random.randint(1000, 9999)}"
                    print(f"ì›Œì»¤ {worker_id}: Could not find post title. Using default name: {post_title}")

                download_dir = os.path.join("download_mana", post_title)
                os.makedirs(download_dir, exist_ok=True)

                html_mana_section = soup.find('section', itemtype='http://schema.org/NewsArticle')
                if html_mana_section:
                    img_tags = html_mana_section.find_all('img')
                    print(f"ì›Œì»¤ {worker_id}: Found {len(img_tags)} images.")

                    for i, img in enumerate(img_tags):
                        img_url = img.get('src')
                        if img_url and '.gif' not in img_url.lower():
                            try:
                                header = {'referer': 'https://manatoki468.net/'}
                                response = requests.get(img_url, stream=True, headers=header, timeout=30)
                                response.raise_for_status()

                                content_type = response.headers.get('Content-Type')
                                ext = mimetypes.guess_extension(content_type) if content_type else os.path.splitext(img_url)[1]
                                if not ext: ext = ".jpg"

                                img_filename = os.path.join(download_dir, f"{i + 1:03d}{ext}")
                                with open(img_filename, 'wb') as f:
                                    for chunk in response.iter_content(1024):
                                        f.write(chunk)
                                print(f"ì›Œì»¤ {worker_id}: Downloaded {img_filename}")
                            except requests.exceptions.RequestException as req_err:
                                print(f"ì›Œì»¤ {worker_id}: Error downloading {img_url}: {req_err}")

                    add_crawled_url(url, post_title)
                    print(f"ì›Œì»¤ {worker_id}: [SUCCESS] í¬ë¡¤ë§ ì™„ë£Œ - {post_title}")
                else:
                    print(f"ì›Œì»¤ {worker_id}: [FAIL] í¬ë¡¤ë§ ì‹¤íŒ¨ - mana_section ì´ ì—†ë‹¤. {url}")
            except Exception as e:
                print(f"ì›Œì»¤ {worker_id}: [FAIL] ì²˜ë¦¬ì¤‘ ì˜¤ë¥˜ ë°œìƒ {url}. {e}")
            finally:
                q.task_done()
                time.sleep(random.randint(1, 3))

    finally:
        if driver:
            driver.quit()
        print(f"ì›Œì»¤ {worker_id}: ì¢…ë£Œ")


def get_target_pages(driver: Driver, target_url) -> list:
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡ì„ ì¡°íšŒ í•©ë‹ˆë‹¤.")
    try:
        driver.get(target_url)
        print(f"Successfully opened {target_url}")
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article[itemprop='articleBody']"))
        )
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        article_body = soup.find('article', itemprop='articleBody')
        if article_body:
            serial_list_div = article_body.find('div', class_='serial-list')
            links = serial_list_div.find_all('a', href=True) if serial_list_div else []
            article_urls = [link['href'] for link in links]
            print("Found articleBody. Extracted URLs:")
            for url in article_urls:
                print(url)
            return article_urls
        else:
            print("ë§Œí™” ëª©ë¡ì´ ì—†ë„¤ìš”.")
            return []
    except Exception as e:
        print(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    return []


def main():
    os.makedirs("download_mana", exist_ok=True)

    target_url = ""
    while not target_url:
        target_url = input("í¬ë¡¤ë§ í•  ë§Œí™” ëª©ë¡ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if not target_url:
            print("URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            confirm = input(f"{target_url} ë§ë‚˜ìš”?(Y/n) ")
            if confirm.lower() not in ["", "y", "yes", "ì˜ˆ"]:
                target_url = ""

    print("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    article_urls = []
    # print("í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ URL ëª©ë¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # article_urls = dummy_url_list

    # ëª©ë¡ì¡°íšŒëŠ” ìº¡ì±  ì—†ìŒ. headless ëª¨ë“œ
    driver = Driver(uc=True, headless=False)
    try:
        article_urls = get_target_pages(driver, target_url)
        if not article_urls:
            print("í¬ë¡¤ë§í•  í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
    finally:
        driver.quit()

    crawl_queue = queue.Queue()
    for url in article_urls:
        crawl_queue.put(url)


    print(f"{crawl_queue.qsize()}ê°œì˜ ì‘ì—…ì„ {NUM_THREADS}ê°œì˜ ìŠ¤ë˜ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    # ë…ì•½(Poison Pill) íŒ¨í„´: ì‘ì—… ì¢…ë£Œë¥¼ ìœ„í•´ ìŠ¤ë ˆë“œ ìˆ˜ë§Œí¼ Noneì„ íì— ì¶”ê°€
    for _ in range(NUM_THREADS):
        crawl_queue.put(None)

    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        futures = [executor.submit(crawl_worker, crawl_queue, i + 1) for i in range(NUM_THREADS)]
        # concurrent.futures.wait(futures)
        crawl_queue.join()

    print("\n\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    # ì¢…ë£Œ
    sys.exit(0)


if __name__ == "__main__":
    main()
